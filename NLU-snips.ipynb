{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snips_nlu import SnipsNLUEngine\n",
    "import io\n",
    "from snips_nlu.default_configs import CONFIG_EN\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLU Snips converter\n",
    "Quick and dirty notebook to:\n",
    "* convert the NLU dataset from CSV to JSON format\n",
    "* train the model\n",
    "* export the model\n",
    "* run the engine\n",
    "\n",
    "A user can skip most steps here if they just want to load the pre-trained model for the whole dataset and test the engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example NLU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the JSON file with the training data\n",
    "\n",
    "NOTE: skip this next cell, unless you want to reproduce the data. A model has already been exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"lights_dataset.json\", \"r\") as file:\n",
    "    nlu_data = json.load(file)\n",
    "\n",
    "print(json.dumps(nlu_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the engine, train the model, and export the data\n",
    "\n",
    "NOTE: skips this next cell unless you want to reproduce the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_engine = SnipsNLUEngine(config=CONFIG_EN)\n",
    "nlu_engine.fit(nlu_data)\n",
    "nlu_engine.persist(\"nlu_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the engine with the exported model and test an utterance (while we are at it, let's get the top 3 matches to have a deeper look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_engine = SnipsNLUEngine.from_path(\"nlu_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = \"turn on the lights in the kitchen\"\n",
    "parsing = loaded_engine.parse(utterance, top_n=3)\n",
    "print(json.dumps(parsing, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive into the CSV file and convert it into Snips JSON training format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV NLU dataset we want to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data_df = pd.read_csv(file_name, sep=';')\n",
    "    data_df = data_df.dropna(axis=0, how='any', subset=['answer_normalised'])\n",
    "    data_df = data_df[data_df['answer_normalised'].str.contains(' ')].reset_index()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_whole_dataset_df = load_data('NLU-Data-Home-Domain-Annotated-All.csv')\n",
    "nlu_whole_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huge block ahead!\n",
    "We need to convert this CSV to snips format similar to the example skill\n",
    "\n",
    "NOTE: the next 3 cell are optional, only run if you want to reproduce the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity = False\n",
    "\n",
    "def utterance_data_process(data):\n",
    "    \"\"\"\n",
    "    We get a list of strings like this one:\n",
    "        ['wake me up at', 'time: five am', '', 'date: this week', '']\n",
    "    \n",
    "    The first one is pure text and from there it alternates between entities and pure text.\n",
    "\n",
    "    For this examples it would be:\n",
    "        Text, Entity, Text, Entity, Text\n",
    "    \"\"\"\n",
    "    global is_entity\n",
    "    if is_entity:\n",
    "        is_entity = not is_entity\n",
    "        splitted = data.split(' : ')\n",
    "        try:\n",
    "            return {\n",
    "                \"entity\": splitted[0],\n",
    "                \"slot_name\": splitted[0],\n",
    "                \"text\": splitted[1]\n",
    "            }\n",
    "\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        is_entity = not is_entity\n",
    "        return {\"text\": data}\n",
    "\n",
    "\n",
    "def split(text):\n",
    "    return re.split(r\"[\\[\\]]\", text)\n",
    "\n",
    "\n",
    "def intents(csv_dataset):\n",
    "    def not_empty(utt):\n",
    "        return utt['text'] != ''\n",
    "\n",
    "    def not_none(utt):\n",
    "        return utt is not None\n",
    "\n",
    "    global is_entity\n",
    "    intents = {}\n",
    "\n",
    "    for row in range(len(csv_dataset[\"scenario\"])):\n",
    "        intent_name = f'{csv_dataset[\"scenario\"][row]}_{csv_dataset[\"intent\"][row]}'\n",
    "\n",
    "        if not intent_name in intents:\n",
    "            intents[intent_name] = {\"utterances\": []}\n",
    "\n",
    "        utterance_data = list(\n",
    "            filter(not_empty,\n",
    "                   filter(not_none,\n",
    "                          map(utterance_data_process,\n",
    "                              split(csv_dataset[\"answer_annotation\"][row])\n",
    "                              )\n",
    "                          )\n",
    "                   )\n",
    "        )\n",
    "        is_entity = False\n",
    "\n",
    "        intents[intent_name][\"utterances\"].append({\"data\": utterance_data})\n",
    "\n",
    "    return intents\n",
    "\n",
    "def extract_entities(utterance):\n",
    "    def seperate_types_and_entities(entity):\n",
    "        split_entity = entity.split(' : ')\n",
    "        entity_type = split_entity[0]\n",
    "        entity_text = split_entity[1]\n",
    "        return (entity_type, entity_text)\n",
    "\n",
    "    entities = re.findall(r'\\[(.*?)\\]', utterance)\n",
    "    \n",
    "    return list(\n",
    "        map(seperate_types_and_entities, entities)\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_entities_to_snips_format(utterance):\n",
    "    def to_snips(entity):\n",
    "        return (entity[0], {\n",
    "            \"value\": entity[1],\n",
    "            \"synonyms\": []\n",
    "        })\n",
    "\n",
    "    return list(map(to_snips, extract_entities(utterance)))\n",
    "\n",
    "\n",
    "def utterances(dataset):\n",
    "    output = {}\n",
    "    for utterance in dataset['answer_annotation']:\n",
    "        for (type, data) in convert_entities_to_snips_format(utterance):\n",
    "            if type not in output:\n",
    "                output[type] = {\n",
    "                    \"automatically_extensible\": bool('true'),\n",
    "                    \"use_synonyms\": bool('true'),\n",
    "                    'data': []\n",
    "                }\n",
    "\n",
    "            output[type]['data'].append(data)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def to_snips(dataset):\n",
    "    return {\n",
    "        \"entities\": utterances(dataset),\n",
    "        \"intents\": intents(dataset),\n",
    "        \"language\": \"en\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_whole_dataset_snips = to_snips(nlu_whole_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export nlu_whole_dataset_snips to json file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlu_whole_dataset_snips.json', 'w') as outfile:\n",
    "    json.dump(nlu_whole_dataset_snips, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "NOTE: Skip this unless you want to reproduce the results. An exported model has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the converted NLU dataset, train the model, and export the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"nlu_whole_dataset_snips.json\", \"r\") as file:\n",
    "    nlu_whole_dataset_snips = json.load(file)\n",
    "\n",
    "nlu_engine = SnipsNLUEngine(config=CONFIG_EN)\n",
    "nlu_engine.fit(nlu_whole_dataset_snips)\n",
    "\n",
    "nlu_engine.persist('nlu_whole_dataset_engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_loaded_engine = SnipsNLUEngine.from_path(\"nlu_whole_dataset_engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing = whole_loaded_engine.parse(\"Can you turn on the lights in the livingroom\", top_n=3)\n",
    "print(json.dumps(parsing, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: f1 benchmark for each intent with full report"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c41e2e0daf193f6a072ee1be75c97a598c9bdbd6986c65a572865f6989cf9914"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
